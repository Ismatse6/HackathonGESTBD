{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d4719",
   "metadata": {},
   "source": [
    "# PDFPLUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02672a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b1791a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasTargetHeaders(tabla, encabezados_objetivo):\n",
    "    \"\"\"\n",
    "    Verifica si la primera fila de una tabla PDF contiene todos los encabezados esperados.\n",
    "\n",
    "    Parámetros:\n",
    "        tabla (list[list[str]]): Lista de filas (cada fila es una lista de celdas).\n",
    "        encabezados_objetivo (set[str]): Conjunto de encabezados requeridos (en minúsculas).\n",
    "\n",
    "    Retorna:\n",
    "        bool: True si la tabla contiene todos los encabezados buscados, False en caso contrario.\n",
    "    \"\"\"\n",
    "    if not tabla or len(tabla) == 0:\n",
    "        return False\n",
    "    encabezados = [str(celda).strip().lower() for celda in tabla[0] if celda]\n",
    "    return encabezados_objetivo.issubset(set(encabezados))\n",
    "\n",
    "def scrapPDFBibliography(ruta, encabezados_objetivo):\n",
    "    \"\"\"\n",
    "    Extrae una tabla específica desde un PDF que contiene los encabezados objetivo.\n",
    "\n",
    "    Parámetros:\n",
    "        ruta (str): Ruta del archivo PDF.\n",
    "        encabezados_objetivo (set[str]): Encabezados que identifican la tabla buscada.\n",
    "\n",
    "    Retorna:\n",
    "        pandas.DataFrame: Tabla encontrada, limpiada y combinada si se extiende en varias páginas.\n",
    "    \"\"\"\n",
    "    tablas_por_pagina = []\n",
    "\n",
    "    # Leer todas las tablas del PDF página por página\n",
    "    with pdfplumber.open(ruta) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            tablas_pagina = []\n",
    "            for tabla in pagina.extract_tables():\n",
    "                if tabla and len(tabla) > 1:\n",
    "                    tablas_pagina.append(tabla)\n",
    "            tablas_por_pagina.append(tablas_pagina)\n",
    "\n",
    "    # Buscar la primera tabla que contenga los encabezados objetivo\n",
    "    tabla_encontrada = None\n",
    "    indice_pagina = -1\n",
    "    indice_tabla = -1\n",
    "\n",
    "    for i, tablas_pag in enumerate(tablas_por_pagina):\n",
    "        for j, tabla in enumerate(tablas_pag):\n",
    "            if hasTargetHeaders(tabla, encabezados_objetivo):\n",
    "                tabla_encontrada = tabla\n",
    "                indice_pagina = i\n",
    "                indice_tabla = j\n",
    "                break\n",
    "        if tabla_encontrada:\n",
    "            break\n",
    "    \n",
    "    # No se encontró tabla válida\n",
    "    if not tabla_encontrada:\n",
    "        return pd.DataFrame()  \n",
    "\n",
    "    # Combinar con tablas de páginas siguientes si es necesario\n",
    "    filas_combinadas = list(tabla_encontrada)\n",
    "    pagina_actual = indice_pagina\n",
    "\n",
    "    while True:\n",
    "        tablas_pag_actual = tablas_por_pagina[pagina_actual]\n",
    "\n",
    "        # Verificar si la tabla actual es la última de la página\n",
    "        if indice_tabla != len(tablas_pag_actual) - 1:\n",
    "            break\n",
    "\n",
    "        # Verificar si hay una página siguiente\n",
    "        if pagina_actual + 1 >= len(tablas_por_pagina):\n",
    "            break\n",
    "\n",
    "        # Verificar si la siguiente página contiene tablas\n",
    "        tablas_pag_siguiente = tablas_por_pagina[pagina_actual + 1]\n",
    "        if not tablas_pag_siguiente:\n",
    "            break\n",
    "\n",
    "        # Tomar la primera tabla de la siguiente página\n",
    "        primera_tabla_siguiente = tablas_pag_siguiente[0]\n",
    "\n",
    "        # Si la siguiente tabla vuelve a tener encabezados, no combinar\n",
    "        if hasTargetHeaders(primera_tabla_siguiente, encabezados_objetivo):\n",
    "            break\n",
    "\n",
    "        # Combinar las filas (sin encabezado repetido)\n",
    "        filas_combinadas.extend(primera_tabla_siguiente)\n",
    "        pagina_actual += 1\n",
    "        indice_tabla = 0\n",
    "\n",
    "    # Crear DataFrame\n",
    "    df = pd.DataFrame(filas_combinadas[1:], columns=filas_combinadas[0])\n",
    "\n",
    "    # Limpiar filas no deseadas\n",
    "    df = df[~df[\"Nombre\"].str.contains(r\"^https?://\", na=False)]\n",
    "    df = df[df[\"Tipo\"] == \"Bibliografía\"]\n",
    "\n",
    "    # Eliminar saltos de línea en columnas\n",
    "    df[\"Nombre\"] = df[\"Nombre\"].str.replace(\"\\n\", \" \", regex=False)\n",
    "    df[\"Observaciones\"] = df[\"Observaciones\"].str.replace(\"\\n\", \" \", regex=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scrapGoogleScholar(name):\n",
    "    \"\"\"\n",
    "    Busca un autor o título en Google Scholar y devuelve información básica del primer resultado.\n",
    "\n",
    "    Parámetros:\n",
    "        name (str): Nombre del autor o texto a buscar.\n",
    "\n",
    "    Retorna:\n",
    "        dict: Contiene título, autores y enlace del primer resultado encontrado.\n",
    "    \"\"\"\n",
    "    url = f\"https://scholar.google.com/scholar?q={quote(name)}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    respuesta = requests.get(url, headers=headers)\n",
    "\n",
    "    if respuesta.status_code != 200:\n",
    "        raise Exception(f\"Error al acceder a Google Scholar: {respuesta.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(respuesta.text, \"html.parser\")\n",
    "    resultado = {}\n",
    "    item = soup.select_one(\".gs_r.gs_or.gs_scl\")\n",
    "\n",
    "    if item:\n",
    "        titulo_elem = item.select_one(\".gs_rt\")\n",
    "        autor_elem = item.select_one(\".gs_a\")\n",
    "        link_elem = titulo_elem.find(\"a\") if titulo_elem else None\n",
    "\n",
    "        titulo = titulo_elem.get_text(strip=True) if titulo_elem else \"\"\n",
    "        autores = autor_elem.get_text(strip=True) if autor_elem else \"\"\n",
    "        enlace = link_elem[\"href\"] if link_elem and link_elem.has_attr(\"href\") else \"\"\n",
    "\n",
    "        # Limpiar etiquetas comunes\n",
    "        for tag in [\"[PDF]\", \"[LIBRO]\", \"[B]\", \"[CITAS]\", \"[C]\", \"[HTML]\"]:\n",
    "            titulo = titulo.replace(tag, \"\")\n",
    "\n",
    "        resultado[\"Título\"] = titulo.strip()\n",
    "        resultado[\"Autores\"] = autores.split(\"-\")[0].strip()\n",
    "        resultado[\"Enlace\"] = enlace\n",
    "\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Guias Docentes\"\n",
    "\n",
    "dfs_scrap = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith(\".pdf\"):  \n",
    "        ruta = os.path.join(directory, file)\n",
    "        df = scrapPDFBibliography(ruta, {\"nombre\", \"tipo\", \"observaciones\"})  \n",
    "        if not df.empty:  \n",
    "            df[\"Guia Docente\"] = file  \n",
    "            dfs_scrap.append(df)\n",
    "\n",
    "df_scrap = pd.concat(dfs_scrap, ignore_index=True)\n",
    "\n",
    "dfs_google = []\n",
    "\n",
    "for nombre in df_scrap['Nombre']:\n",
    "    try:\n",
    "        dict_bibliografia = scrapGoogleScholar(nombre)\n",
    "        df_temp = pd.DataFrame([dict_bibliografia])\n",
    "        dfs_google.append(df_temp)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con {nombre}: {e}\")\n",
    "\n",
    "    time.sleep(random.uniform(5, 15))\n",
    "\n",
    "if dfs_google != []:\n",
    "    df_google = pd.concat(dfs_google, ignore_index=True)\n",
    "    df_google.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ad2fb",
   "metadata": {},
   "source": [
    "# Conectar a PostGreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9c85a",
   "metadata": {},
   "source": [
    "CREATE TABLE Asignaturas (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    nombre VARCHAR(150) NOT NULL,\n",
    "    coordinador VARCHAR(100) NOT NULL,\n",
    "    numero_creditos INT NOT NULL,\n",
    "    agno_academico VARCHAR(100) NOT NULL,\n",
    "    direccion_url VARCHAR(255) NOT NULL,\n",
    "    semestre VARCHAR(100) NOT NULL,\n",
    "    idioma VARCHAR(100) NOT NULL,\n",
    "    id_guia_docente INT NOT NULL,\n",
    "    titulacion INT NOT NULL,\n",
    "    FOREIGN KEY (titulacion) REFERENCES Titulaciones(id)\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE Bibliografias (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    asignatura_id INT, \n",
    "    nombre VARCHAR(100),\n",
    "    autores VARCHAR(100),\n",
    "    url VARCHAR(200),\n",
    "    FOREIGN KEY (asignatura_id) REFERENCES Asignaturas(id)  \n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80b790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "usuario = \"userPSQL\"\n",
    "contraseña = \"passPSQL\"\n",
    "host = \"localhost\"  \n",
    "puerto = \"5432\"\n",
    "base_datos = \"postgres\"\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{usuario}:{contraseña}@{host}:{puerto}/{base_datos}\"\n",
    ")\n",
    "\n",
    "df_google.to_sql('Bibliografias', engine, if_exists=\"replace\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
